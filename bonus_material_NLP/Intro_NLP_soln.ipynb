{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Code - Introduction to Natural Language Processing\n",
    "\n",
    "<p><a name=\"sections\"></a></p>\n",
    "\n",
    "\n",
    "## Sections\n",
    "\n",
    "- <a href=\"#prepare\">Prepare Our Environment</a><br>\n",
    "- <a href=\"#corpus\">Creating Corpus</a><br>\n",
    "    - <a href=\"#stemming\">Stemming</a><br>\n",
    "    - <a href=\"#ex1\">Exercise: Lemmatization</a><br>\n",
    "- <a href=\"#pos\">POS Tag</a><br>\n",
    "    - <a href=\"#ex2\">Exercise: Lemmatization with POS Tag</a><br>\n",
    "- <a href=\"#chunk\">Chunk</a><br>\n",
    "    - <a href=\"#ex3\">Exercise: Syntax Tree/ Chunking</a><br>\n",
    "- <a href=\"#classify\">Text Classification</a><br>\n",
    "    - <a href=\"#ex4\">Exercise: Classify the Testing Set</a><br>\n",
    "- <a href=\"#lda\">Brief Introduction to LDA</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"prepare\"></a></p>\n",
    "## Prepare Our Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "# Python 2 and 3: alternative 4\n",
    "try:\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen\n",
    "import re\n",
    "import nltk\n",
    "from nltk import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download everything from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package hmm_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package panlex_lite to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package panlex_lite is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/rheineke/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"corpus\"></a></p>\n",
    "## Creating Corpus\n",
    "\n",
    "The first step is to gather a data set to work on. In this case, we will use the text from The Great Gatsby by F. Scott Fitzgerald. It's freely available on the Project Gutenberg.\n",
    "\n",
    "- Scrape the text for the novel, clean it up using regex, and put into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://gutenberg.net.au/ebooks02/0200041.txt\"\n",
    "text = urlopen(url).read()\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "cleantext = BeautifulSoup.get_text(soup)\n",
    "cleantext = re.sub('\\s+', ' ', cleantext).strip()\n",
    "cleantext = cleantext.lower()\n",
    "cleantext = re.sub('[.:\\',\\-!;\"()?]', '', cleantext).strip()\n",
    "corpus = cleantext.split(\" \")\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cleaning the corpus: tokenization **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(corpus)): \n",
    "    if corpus[x] == \"chapter\":\n",
    "        break_number_1 = x \n",
    "        break\n",
    "for x in range((len(corpus)-1), 0, -1): \n",
    "    if corpus[x] == \"end\":\n",
    "        break_number_2 = x + 1 \n",
    "        break\n",
    "\n",
    "corpus = corpus[break_number_1 : break_number_2]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"stemming\"></a></p>\n",
    "### Stemming\n",
    "- This is probably the first time you'll see a Python object of some **tasks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer() # Create our stemmer \n",
    "stemmed_corpus = [stemmer.stem(word) for word in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential problems**\n",
    "- What is the result of the code below? what kind of problem do you suspect could arise? How can we fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['monkey',\n",
       " 'possess',\n",
       " 'possess',\n",
       " 'dog',\n",
       " 'eat',\n",
       " 'constitut',\n",
       " 'worthi',\n",
       " 'relat',\n",
       " 'had',\n",
       " 'wa',\n",
       " 'tie',\n",
       " 'ive']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_stemmed = [\"monkeys\",\"possesses\", \"possess\", \"dogs\", \"eating\",\n",
    "\"constitutional\", \"worthy\", \"relatable\", \"had\", \"was\", \"ties\", \"ive\"]\n",
    "[stemmer.stem(word) for word in to_be_stemmed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"ex1\"></a></p>\n",
    "### Exercise: Lemmatization\n",
    "\n",
    "- To reduce some of the problem, we might want to add more steps before stemming. For example, we can lemmatize in advance. Look up the documentation, find the function you need for lemmatization, then apply it (in the correct way) to the words below.\n",
    "    - 'tips', 'criteria', 'minima'\n",
    "    - 'was', 'wore', 'caught'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtizer=nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tip\n",
      "criterion\n",
      "minimum\n"
     ]
    }
   ],
   "source": [
    "#### Your code here\n",
    "print(lmtizer.lemmatize('tips'))\n",
    "print(lmtizer.lemmatize('criteria'))\n",
    "print(lmtizer.lemmatize('minima'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa\n",
      "wore\n",
      "caught\n"
     ]
    }
   ],
   "source": [
    "print(lmtizer.lemmatize('was'))\n",
    "print(lmtizer.lemmatize('wore'))\n",
    "print(lmtizer.lemmatize('caught'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa\n",
      "be\n",
      "\n",
      "wore\n",
      "wear\n",
      "\n",
      "caught\n",
      "catch\n"
     ]
    }
   ],
   "source": [
    "print(lmtizer.lemmatize('was'))\n",
    "print(lmtizer.lemmatize('was', 'v')+'\\n')\n",
    "\n",
    "print(lmtizer.lemmatize('wore'))\n",
    "print(lmtizer.lemmatize('wore', 'v')+'\\n')\n",
    "\n",
    "print(lmtizer.lemmatize('caught'))\n",
    "print(lmtizer.lemmatize('caught', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is crucial to decide what class each word belongs to (noun, verb or adjective). We will see how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"pos\"></a></p>\n",
    "## POS Tag\n",
    "\n",
    "- In order to look up definitions for all of the parts of speech, you can run this command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A different way to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " \"didn't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't\n",
    "feel very good.\"\"\"\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compare to the old tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_sentence = re.sub('\\s+', ' ', sentence).strip()\n",
    "clean_sentence = clean_sentence.lower()\n",
    "clean_sentence = re.sub('[.:\\',\\-!;\"()?]', \"\", clean_sentence).strip()\n",
    "old_tokens = clean_sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " ('Arthur', 'NNP'),\n",
       " (\"didn't\", 'NN'),\n",
       " ('feel', 'VB'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"ex2\"></a></p>\n",
    "#### Exercise: Lemmatization with POS Tag\n",
    "\n",
    "- Consider our `to_be_stemmed` list again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monkeys', 'possesses', 'possess', 'dogs', 'eating', 'constitutional', 'worthy', 'relatable', 'had', 'was', 'ties', 'ive']\n"
     ]
    }
   ],
   "source": [
    "print(to_be_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only need very basic tags (noun, verb and adjective), try passing `'universal'` to the `pos_tag` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Your code here\n",
    "tagged = nltk.pos_tag(to_be_stemmed, tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lemmatize the sentence above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monkeys', 'NOUN'),\n",
       " ('possesses', 'VERB'),\n",
       " ('possess', 'ADJ'),\n",
       " ('dogs', 'NOUN'),\n",
       " ('eating', 'VERB'),\n",
       " ('constitutional', 'ADJ'),\n",
       " ('worthy', 'NOUN'),\n",
       " ('relatable', 'NOUN'),\n",
       " ('had', 'VERB'),\n",
       " ('was', 'VERB'),\n",
       " ('ties', 'NOUN'),\n",
       " ('ive', 'ADJ')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here\n",
    "mapping={'NOUN':'n', 'VERB':'v', 'ADJ':'a'}\n",
    "tagged=map(lambda tup: [tup[0], mapping[tup[1]]], tagged)\n",
    "to_be_stemmed2 = map(lambda x: lmtizer.lemmatize(*x), tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stem it. Compare the result from the old stemming method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monkey', 'possess', 'possess', 'dog', 'eat', 'constitut', 'worthi', 'relat', 'had', 'wa', 'tie', 'ive']\n",
      "['monkey', 'possess', 'possess', 'dog', 'eat', 'constitut', 'worthi', 'relat', 'have', 'be', 'tie', 'ive']\n"
     ]
    }
   ],
   "source": [
    "#### Your code here\n",
    "print([stemmer.stem(word) for word in to_be_stemmed])\n",
    "print([stemmer.stem(word) for word in to_be_stemmed2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"chunk\"></a></p>\n",
    "## Chunk\n",
    "\n",
    "Defining patterns in POS helps, for example, to find phrases in a corpus -- especially noun phrases. Let's consider the sentence below (This example os from [The nltk book](http://www.nltk.org/book/ch07.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The little yellow dog barked at the cat.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The little yellow dog barked at the cat.'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As before we tokenize and tag the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens   = nltk.word_tokenize(sentence.lower())\n",
    "tagged   = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we may use the `RegexpParser` (Regular expression parser) function to do **chunking**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The result actually admits a tree structure and can be seen with the `draw()` function (The sketching is not inline here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"ex3\"></a></p>\n",
    "### Exercise: Syntax Tree/ Chunking\n",
    "\n",
    "Try chunking:\n",
    "- `'We live in New York City'` with `grammar = \"NP: {<NNP>+}\"`.\n",
    "- `'I went to a baseball game'` with `grammar = \"NP: {<NN>+}\"`.\n",
    "- Identify th esubject of the sentence: `'the young tall guy looked at the window'`.\n",
    "\n",
    "You can visualize the structure with the `draw()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP live/VBP in/IN (NP New/NNP York/NNP City/NNP))\n"
     ]
    }
   ],
   "source": [
    "#### Your code here\n",
    "sentence = 'We live in New York City'\n",
    "tokens   = nltk.word_tokenize(sentence)\n",
    "tagged   = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<NNP>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP went/VBD to/TO a/DT (NP baseball/NN game/NN) ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I went to a baseball game.'\n",
    "tokens   = nltk.word_tokenize(sentence)\n",
    "tagged   = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<NN>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT young/JJ tall/NN)\n",
      "  (NP guy/NN)\n",
      "  looked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT window/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the young tall guy looked at the window'\n",
    "tokens   = nltk.word_tokenize(sentence)\n",
    "tagged   = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the same processing for this sentence:\n",
    "\n",
    "- *** Time flies like an arrow; fruit flies like a banana ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Time/NNP\n",
      "  flies/NNS\n",
      "  like/IN\n",
      "  an/DT\n",
      "  (NP arrow/NN)\n",
      "  ;/:\n",
      "  fruit/CC\n",
      "  flies/NNS\n",
      "  like/IN\n",
      "  a/DT\n",
      "  (NP banana/NN))\n"
     ]
    }
   ],
   "source": [
    "#### Your code here\n",
    "\n",
    "sentence = 'Time flies like an arrow; fruit flies like a banana'\n",
    "tokens   = nltk.word_tokenize(sentence)\n",
    "tagged   = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<NN>}\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"classify\"></a></p>\n",
    "## Text Classification\n",
    "\n",
    "### creating text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = [('I love this book', 'positive'),\n",
    "              ('This food is amazing', 'positive'),\n",
    "              ('I feel great this morning', 'positive'),\n",
    "              ('I am so excited about the party', 'positive'),\n",
    "              ('He is my best friend', 'positive')]\n",
    "neg_tweets = [('I do not like this book', 'negative'),\n",
    "              ('This food is horrible', 'negative'),\n",
    "              ('I feel tired this morning', 'negative'),\n",
    "              ('I am not looking forward to the party', 'negative'),\n",
    "              ('He is my enemy', 'negative')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['love', 'this', 'book'], 'positive'),\n",
       " (['this', 'food', 'amazing'], 'positive'),\n",
       " (['feel', 'great', 'this', 'morning'], 'positive'),\n",
       " (['excited', 'about', 'the', 'party'], 'positive'),\n",
       " (['best', 'friend'], 'positive'),\n",
       " (['not', 'like', 'this', 'book'], 'negative'),\n",
       " (['this', 'food', 'horrible'], 'negative'),\n",
       " (['feel', 'tired', 'this', 'morning'], 'negative'),\n",
       " (['not', 'looking', 'forward', 'the', 'party'], 'negative'),\n",
       " (['enemy'], 'negative')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for (words, sentiment) in pos_tweets + neg_tweets:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    tweets.append((words_filtered, sentiment))\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets = [\n",
    "(['feel', 'happy', 'this', 'morning'], 'positive'), (['larry', 'friend'], 'positive'),\n",
    "(['not', 'like', 'that', 'man'], 'negative'), (['house', 'not', 'great'], 'negative'),\n",
    "(['your', 'song', 'annoying'], 'negative')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a set of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_in_tweets(tweets): \n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words) \n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets) \n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "tweet = 'Larry is my friend'\n",
    "print(classifier.classify(extract_features(tweet.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"ex4\"></a></p>\n",
    "### Exercise: Classify the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['positive', 'positive', 'negative', 'negative', 'positive']\n",
      "Actual:    ['positive', 'positive', 'negative', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "#### Your code here\n",
    "test_set=[extract_features(lst) for lst in [tup[0] for tup in test_tweets]]\n",
    "print('Predicted: ' + str([classifier.classify(instance) for instance in test_set]))\n",
    "print('Actual:    ' + str([tup[1] for tup in test_tweets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"lda\"></a></p>\n",
    "### Brief Introduction to Latent Dirichlet Allocation\n",
    "\n",
    "In natural language processing, latent Dirichlet allocation (LDA) is an unsupervised method that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. \n",
    "\n",
    "For example, a **topic** is a common latent variable produced by LDA and used to characterise a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we demonstrate, with the [20 newsgroups dataset](http://scikit-learn.org/stable/datasets/), how we can:\n",
    " - Extract **term frequency** feature from the corpus.\n",
    " - Fit the LDA model, return the **topics**, and then inspect each topic.\n",
    " - Finally we inspect the relation between the topics and the particular documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "CPU times: user 2.64 s, sys: 123 ms, total: 2.76 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "%time dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=remove)\n",
    "data_samples = dataset.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the term frequency for the selected words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_transform returns: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 703)\t1\n",
      "  (0, 422)\t1\n",
      "  (0, 502)\t1\n",
      "  (0, 554)\t1\n",
      "  (0, 146)\t1\n",
      "  (0, 573)\t1\n",
      "  (0, 424)\t1\n",
      "  (0, 748)\t1\n",
      "  (0, 762)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 760)\t1\n",
      "  (0, 745)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 914)\t1\n",
      "  (0, 535)\t1\n",
      "  (0, 444)\t1\n",
      "  (0, 985)\t1\n",
      "  (0, 497)\t2\n",
      "  (0, 714)\t1\n",
      "  (0, 589)\t4\n",
      "  (0, 854)\t1\n",
      "  (0, 303)\t1\n",
      "  (0, 860)\t1\n",
      "  (0, 872)\t1\n",
      "  (1, 321)\t1\n",
      "  :\t:\n",
      "  (11313, 758)\t1\n",
      "  (11313, 747)\t1\n",
      "  (11313, 840)\t1\n",
      "  (11313, 637)\t1\n",
      "  (11313, 210)\t1\n",
      "  (11313, 560)\t1\n",
      "  (11313, 646)\t2\n",
      "  (11313, 529)\t1\n",
      "  (11313, 421)\t3\n",
      "  (11313, 550)\t2\n",
      "  (11313, 773)\t1\n",
      "  (11313, 995)\t1\n",
      "  (11313, 434)\t1\n",
      "  (11313, 931)\t1\n",
      "  (11313, 191)\t1\n",
      "  (11313, 207)\t1\n",
      "  (11313, 954)\t1\n",
      "  (11313, 516)\t1\n",
      "  (11313, 540)\t1\n",
      "  (11313, 321)\t1\n",
      "  (11313, 508)\t2\n",
      "  (11313, 88)\t1\n",
      "  (11313, 359)\t1\n",
      "  (11313, 422)\t1\n",
      "  (11313, 303)\t1\n"
     ]
    }
   ],
   "source": [
    "print('fit_transform returns: {}'.format(type(tf)))\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the method toarray(...) on this sparse matrix object to get a familiar numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(11314, 1000)\n",
      "[0 0 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(tf.toarray()))\n",
    "print(tf.toarray().shape)\n",
    "print(tf.toarray()[11313, 300: 310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['development',\n",
       " 'device',\n",
       " 'devices',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names()[300: 310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start to fit the LDA (Latent Dirichlet Allocatcation) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_features=1000...\n",
      "Fit done in 19.837s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, n_features=%d...\"\n",
    "      % n_features)\n",
    "kwds = dict(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0)\n",
    "lda = LatentDirichletAllocation(**kwds)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"Fit done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `lda` return to us is an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.03197977e-01,   3.71351204e+02,   1.11548432e-01, ...,\n",
       "          5.56365604e+00,   1.72149288e+02,   7.44101756e+01],\n",
       "       [  1.17592487e-01,   1.04091218e-01,   1.00069609e-01, ...,\n",
       "          1.16151717e+02,   7.11178897e+00,   1.00269719e-01],\n",
       "       [  9.18690074e+00,   7.36980863e+00,   8.92878098e+00, ...,\n",
       "          2.87613192e+00,   1.49034051e-01,   1.00075326e-01],\n",
       "       ..., \n",
       "       [  1.19368426e-01,   1.16239615e-01,   1.05411754e-01, ...,\n",
       "          1.00057552e-01,   1.06259394e-01,   1.00161900e-01],\n",
       "       [  1.05681809e-01,   4.58678916e+01,   1.00492572e-01, ...,\n",
       "          3.01590736e+02,   6.37732768e+01,   1.88298644e+02],\n",
       "       [  1.06484645e+03,   2.48463781e+02,   2.14078518e+02, ...,\n",
       "          1.31786491e-01,   4.90524664e+01,   2.93634961e+01]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array consists of 10 row and 1000 columns, because we specify that we want 10 topics and each topic is represented by the **distribution** of the one thousand words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `print_top_words` function, we extract the most frequent words from each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "Topic #0:\n",
      "people gun armenian armenians war turkish states israel said children jews 000 state new guns israeli vs military years american\n",
      "\n",
      "Topic #1:\n",
      "government people law mr use president don think right public make state going privacy private security know new rights want\n",
      "\n",
      "Topic #2:\n",
      "space program output entry data nasa use science research build section center launch time high earth year rules long satellite\n",
      "\n",
      "Topic #3:\n",
      "key car chip used keys bike use bit clipper number phone like cars just engine ground des algorithm good secret\n",
      "\n",
      "Topic #4:\n",
      "edu file com available mail ftp files information image send list use version server email pub software cs code window\n",
      "\n",
      "Topic #5:\n",
      "god people does jesus say think believe don know just way like true question life time christian did point bible\n",
      "\n",
      "Topic #6:\n",
      "windows use drive thanks does problem know card like using db scsi dos disk bit need pc memory mac work\n",
      "\n",
      "Topic #7:\n",
      "ax max b8f g9v a86 pl 145 1d9 0t 34u 1t 3t giz bhj wm 2di 75u 2tm cx bxn\n",
      "\n",
      "Topic #8:\n",
      "just don like think know good time ve people said year did didn got ll going way game really team\n",
      "\n",
      "Topic #9:\n",
      "10 00 25 15 12 20 11 14 17 16 13 30 24 50 18 19 27 40 21 new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\\n\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection\n",
    "\n",
    "We then randomly select text from the dataset and inspect how lda predict mixture of topic to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anyone have the AL individual stats or where i can find them?\n",
      "----------------------------------------------------------------------------------------\n",
      "[[ 0.0333454   0.35310095  0.03333861  0.03333575  0.03334173  0.03333927\n",
      "   0.03334062  0.03333778  0.03334672  0.38017317]]\n"
     ]
    }
   ],
   "source": [
    "indx = np.random.choice(tf.shape[0], 1)[0]\n",
    "text = data_samples[indx]\n",
    "\n",
    "print(text)\n",
    "print('-'*88)\n",
    "print(lda.transform(tf[indx,:] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people gun armenian armenians war turkish states israel said children jews 000 state new guns israeli vs military years american\n",
      "----------------------------------------------------------------------------------------\n",
      "government people law mr use president don think right public make state going privacy private security know new rights want\n",
      "----------------------------------------------------------------------------------------\n",
      "just don like think know good time ve people said year did didn got ll going way game really team\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "topic_lst= [0, 1, -2] \n",
    "\n",
    "for topic in topic_lst:\n",
    "    print(' '.join([tf_feature_names[i] for i in lda.components_[topic].argsort()[:-n_top_words - 1:-1]]))\n",
    "    print('-'*88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
